{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT STATEMENTS\n",
    "\n",
    "#Import Python packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Import Snowflake modules\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from pyspark.sql.functions import year\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "# import matplotlib and seaborn to plot charts and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get account credentials from a json file\n",
    "with open(\"data_scientist_auth.json\") as f:\n",
    "    data = json.load(f)\n",
    "    username = data[\"username\"]\n",
    "    password = data[\"password\"]\n",
    "    account = data[\"account\"]\n",
    "\n",
    "# Specify connection parameters\n",
    "connection_parameters = {\n",
    "    \"account\": account,\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"role\": \"TASTY_BI\",\n",
    "    \"warehouse\": \"TASTY_BI_WH\",\n",
    "    \"database\": \"frostbyte_tasty_bytes\",\n",
    "    \"schema\": \"raw_pos\",\n",
    "}\n",
    "\n",
    "# Create Snowpark session\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve order details usa table from snowflake\n",
    "order_detail = session.table(\"frostbyte_tasty_bytes.raw_pos.ORDER_DETAIL\")\n",
    "order_header = session.table(\"frostbyte_tasty_bytes.raw_pos.ORDER_HEADER\")\n",
    "location_table = session.table(\"frostbyte_tasty_bytes.raw_pos.LOCATION\")\n",
    "menu_table = session.table(\"frostbyte_tasty_bytes.raw_pos.MENU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of LOCATION_IDs where the COUNTRY column's value is 'United States'\n",
    "## Filter the 'location_table' where the 'COUNTRY' column is 'United States'\n",
    "filtered_location_table = location_table.filter(location_table['COUNTRY'] == 'United States')\n",
    "\n",
    "## Select the 'LOCATION_ID' column from the filtered DataFrame\n",
    "location_id_df = filtered_location_table.select('LOCATION_ID')\n",
    "\n",
    "## Convert the 'LOCATION_ID' column to a Python list\n",
    "location_id_list = location_id_df.collect()\n",
    "\n",
    "## Extract the values from the DataFrame and convert them to a list\n",
    "location_id_list = [row['LOCATION_ID'] for row in location_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two tables using the 'ORDER_ID' column as the common key\n",
    "merged_df = order_detail.join(order_header, on='ORDER_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where the LOCATION_ID is for United States\n",
    "merged_df = merged_df.filter(F.col('LOCATION_ID').isin(location_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"ORDER_DETAIL_ID\"  |\"MENU_ITEM_ID\"  |\"l_3yd5_DISCOUNT_ID\"  |\"LINE_NUMBER\"  |\"QUANTITY\"  |\"UNIT_PRICE\"  |\"PRICE\"  |\"ORDER_ITEM_DISCOUNT_AMOUNT\"  |\"TRUCK_ID\"  |\"LOCATION_ID\"  |\"CUSTOMER_ID\"  |\"r_k81b_DISCOUNT_ID\"  |\"SHIFT_ID\"  |\"SHIFT_START_TIME\"  |\"SHIFT_END_TIME\"  |\"ORDER_CHANNEL\"  |\"ORDER_TS\"           |\"SERVED_TS\"  |\"ORDER_CURRENCY\"  |\"ORDER_AMOUNT\"  |\"ORDER_TAX_AMOUNT\"  |\"ORDER_DISCOUNT_AMOUNT\"  |\"ORDER_TOTAL\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|440500500   |852793972          |81              |NULL                  |0              |1           |12.0000       |12.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 11:48:50  |NULL         |USD               |12.0000         |NULL                |NULL                     |12.0000        |\n",
      "|440500523   |852794048          |85              |NULL                  |3              |1           |3.0000        |3.0000   |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 11:56:12  |NULL         |USD               |64.0000         |NULL                |NULL                     |64.0000        |\n",
      "|440500523   |852794045          |82              |NULL                  |0              |2           |15.0000       |30.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 11:56:12  |NULL         |USD               |64.0000         |NULL                |NULL                     |64.0000        |\n",
      "|440500523   |852794047          |84              |NULL                  |2              |2           |2.0000        |4.0000   |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 11:56:12  |NULL         |USD               |64.0000         |NULL                |NULL                     |64.0000        |\n",
      "|440500523   |852794046          |83              |NULL                  |1              |3           |9.0000        |27.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 11:56:12  |NULL         |USD               |64.0000         |NULL                |NULL                     |64.0000        |\n",
      "|440500614   |852794270          |81              |NULL                  |0              |1           |12.0000       |12.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 12:40:53  |NULL         |USD               |12.0000         |NULL                |NULL                     |12.0000        |\n",
      "|440500673   |852794432          |83              |NULL                  |2              |1           |9.0000        |9.0000   |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 13:10:58  |NULL         |USD               |65.0000         |NULL                |NULL                     |65.0000        |\n",
      "|440500673   |852794431          |82              |NULL                  |1              |2           |15.0000       |30.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 13:10:58  |NULL         |USD               |65.0000         |NULL                |NULL                     |65.0000        |\n",
      "|440500673   |852794433          |84              |NULL                  |3              |1           |2.0000        |2.0000   |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 13:10:58  |NULL         |USD               |65.0000         |NULL                |NULL                     |65.0000        |\n",
      "|440500673   |852794430          |81              |NULL                  |0              |2           |12.0000       |24.0000  |NULL                          |68          |3161.0         |NULL           |NULL                  |200514628   |08:00:00            |14:00:00          |NULL             |2021-09-10 13:10:58  |NULL         |USD               |65.0000         |NULL                |NULL                     |65.0000        |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_df.select(\"MENU_ITEM_ID\", \"ORDER_ID\", \"QUANTITY\", \"UNIT_PRICE\", \"PRICE\", \"ORDER_TS\", \"ORDER_TOTAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_ID\"  |\"ORDER_ID\"  |\"QUANTITY\"  |\"UNIT_PRICE\"  |\"PRICE\"  |\"ORDER_TS\"           |\"ORDER_TOTAL\"  |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "|71              |444016143   |1           |9.0000        |9.0000   |2022-04-06 16:04:17  |23.0000        |\n",
      "|72              |444016144   |1           |7.0000        |7.0000   |2022-04-06 16:04:58  |7.0000         |\n",
      "|75              |444016145   |1           |3.0000        |3.0000   |2022-04-06 16:05:07  |61.0000        |\n",
      "|74              |444016145   |2           |2.0000        |4.0000   |2022-04-06 16:05:07  |61.0000        |\n",
      "|73              |444016145   |2           |12.0000       |24.0000  |2022-04-06 16:05:07  |61.0000        |\n",
      "|72              |444016145   |3           |7.0000        |21.0000  |2022-04-06 16:05:07  |61.0000        |\n",
      "|71              |444016145   |1           |9.0000        |9.0000   |2022-04-06 16:05:07  |61.0000        |\n",
      "|73              |444016146   |1           |12.0000       |12.0000  |2022-04-06 16:05:11  |21.0000        |\n",
      "|71              |444016146   |1           |9.0000        |9.0000   |2022-04-06 16:05:11  |21.0000        |\n",
      "|71              |444016147   |1           |9.0000        |9.0000   |2022-04-06 16:05:28  |9.0000         |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "SnowparkFetchDataException",
     "evalue": "(1406): Failed to fetch a Pandas Dataframe. The error is: Unable to allocate 6.19 MiB for an array with shape (2, 405760) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:381\u001b[0m, in \u001b[0;36mServerConnection._to_data_or_iter\u001b[1;34m(self, results_cursor, to_pandas, to_iter)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    372\u001b[0m     data_or_iter \u001b[39m=\u001b[39m (\n\u001b[0;32m    373\u001b[0m         \u001b[39mmap\u001b[39m(\n\u001b[0;32m    374\u001b[0m             functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m    375\u001b[0m                 _fix_pandas_df_integer, results_cursor\u001b[39m=\u001b[39mresults_cursor\n\u001b[0;32m    376\u001b[0m             ),\n\u001b[0;32m    377\u001b[0m             results_cursor\u001b[39m.\u001b[39mfetch_pandas_batches(),\n\u001b[0;32m    378\u001b[0m         )\n\u001b[0;32m    379\u001b[0m         \u001b[39mif\u001b[39;00m to_iter\n\u001b[0;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m _fix_pandas_df_integer(\n\u001b[1;32m--> 381\u001b[0m             results_cursor\u001b[39m.\u001b[39;49mfetch_pandas_all(), results_cursor\n\u001b[0;32m    382\u001b[0m         )\n\u001b[0;32m    383\u001b[0m     )\n\u001b[0;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m NotSupportedError:\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\cursor.py:1117\u001b[0m, in \u001b[0;36mSnowflakeCursor.fetch_pandas_all\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_telemetry_job_data(\n\u001b[0;32m   1115\u001b[0m     TelemetryField\u001b[39m.\u001b[39mPANDAS_FETCH_ALL, TelemetryData\u001b[39m.\u001b[39mTRUE\n\u001b[0;32m   1116\u001b[0m )\n\u001b[1;32m-> 1117\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_result_set\u001b[39m.\u001b[39;49m_fetch_pandas_all(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\result_set.py:187\u001b[0m, in \u001b[0;36mResultSet._fetch_pandas_all\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m dataframes:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas\u001b[39m.\u001b[39;49mconcat(\n\u001b[0;32m    188\u001b[0m         dataframes,\n\u001b[0;32m    189\u001b[0m         ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Don't keep in result batch indexes\u001b[39;49;00m\n\u001b[0;32m    190\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[39m# Empty dataframe\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m using_copy_on_write():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:242\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    243\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:581\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    579\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m--> 581\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[0;32m    582\u001b[0m     ju\u001b[39m.\u001b[39mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39mempty_dtype, upcasted_na\u001b[39m=\u001b[39mupcasted_na)\n\u001b[0;32m    583\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[0;32m    584\u001b[0m ]\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    587\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:582\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    579\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m    581\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 582\u001b[0m     ju\u001b[39m.\u001b[39;49mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39;49mempty_dtype, upcasted_na\u001b[39m=\u001b[39;49mupcasted_na)\n\u001b[0;32m    583\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[0;32m    584\u001b[0m ]\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    587\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:567\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[39mfor\u001b[39;00m ax, indexer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindexers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 567\u001b[0m         values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(values, indexer, axis\u001b[39m=\u001b[39;49max)\n\u001b[0;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mempty(out_shape, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.19 MiB for an array with shape (2, 405760) and data type float64",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSnowparkFetchDataException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_df \u001b[39m=\u001b[39m final_df\u001b[39m.\u001b[39;49mto_pandas()\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:139\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    138\u001b[0m     \u001b[39mwith\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39mquery_history() \u001b[39mas\u001b[39;00m query_history:\n\u001b[1;32m--> 139\u001b[0m         result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    140\u001b[0m     plan \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_select_statement \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_plan\n\u001b[0;32m    141\u001b[0m     api_calls \u001b[39m=\u001b[39m [\n\u001b[0;32m    142\u001b[0m         \u001b[39m*\u001b[39mplan\u001b[39m.\u001b[39mapi_calls,\n\u001b[0;32m    143\u001b[0m         {TelemetryField\u001b[39m.\u001b[39mNAME\u001b[39m.\u001b[39mvalue: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame.\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    144\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:743\u001b[0m, in \u001b[0;36mDataFrame.to_pandas\u001b[1;34m(self, statement_params, block, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[39m@df_collect_api_telemetry\u001b[39m\n\u001b[0;32m    718\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_pandas\u001b[39m(\n\u001b[0;32m    719\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m    724\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame\u001b[39m\u001b[39m\"\u001b[39m, AsyncJob]:\n\u001b[0;32m    725\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m    Executes the query representing this DataFrame and returns the result as a\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m    `Pandas DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39m        :func:`Session.sql` can only be a SELECT statement.\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    744\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan,\n\u001b[0;32m    745\u001b[0m         to_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    746\u001b[0m         block\u001b[39m=\u001b[39;49mblock,\n\u001b[0;32m    747\u001b[0m         data_type\u001b[39m=\u001b[39;49m_AsyncResultType\u001b[39m.\u001b[39;49mPANDAS,\n\u001b[0;32m    748\u001b[0m         _statement_params\u001b[39m=\u001b[39;49mcreate_or_update_statement_params_with_query_tag(\n\u001b[0;32m    749\u001b[0m             statement_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mquery_tag, SKIP_LEVELS_TWO\n\u001b[0;32m    750\u001b[0m         ),\n\u001b[0;32m    751\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     \u001b[39m# if the returned result is not a pandas dataframe, raise Exception\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     \u001b[39m# this might happen when calling this method with non-select commands\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[39m# e.g., session.sql(\"create ...\").to_pandas()\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m block:\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:418\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m is_in_stored_procedure() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m block:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsync query is not supported in stored procedure yet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[1;32m--> 418\u001b[0m result_set, result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_set(\n\u001b[0;32m    419\u001b[0m     plan,\n\u001b[0;32m    420\u001b[0m     to_pandas,\n\u001b[0;32m    421\u001b[0m     to_iter,\n\u001b[0;32m    422\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    423\u001b[0m     block\u001b[39m=\u001b[39;49mblock,\n\u001b[0;32m    424\u001b[0m     data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[0;32m    425\u001b[0m     log_on_exception\u001b[39m=\u001b[39;49mlog_on_exception,\n\u001b[0;32m    426\u001b[0m     case_sensitive\u001b[39m=\u001b[39;49mcase_sensitive,\n\u001b[0;32m    427\u001b[0m )\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    429\u001b[0m     \u001b[39mreturn\u001b[39;00m result_set\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:98\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     97\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     99\u001b[0m     \u001b[39mexcept\u001b[39;00m snowflake\u001b[39m.\u001b[39mconnector\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mProgrammingError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    100\u001b[0m         query \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:521\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mfor\u001b[39;00m holder, id_ \u001b[39min\u001b[39;00m placeholders\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    520\u001b[0m     final_query \u001b[39m=\u001b[39m final_query\u001b[39m.\u001b[39mreplace(holder, id_)\n\u001b[1;32m--> 521\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_query(\n\u001b[0;32m    522\u001b[0m     final_query,\n\u001b[0;32m    523\u001b[0m     to_pandas,\n\u001b[0;32m    524\u001b[0m     to_iter \u001b[39mand\u001b[39;49;00m (i \u001b[39m==\u001b[39;49m \u001b[39mlen\u001b[39;49m(plan\u001b[39m.\u001b[39;49mqueries) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[0;32m    525\u001b[0m     is_ddl_on_temp_object\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mis_ddl_on_temp_object,\n\u001b[0;32m    526\u001b[0m     block\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_last,\n\u001b[0;32m    527\u001b[0m     data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[0;32m    528\u001b[0m     async_job_plan\u001b[39m=\u001b[39;49mplan,\n\u001b[0;32m    529\u001b[0m     log_on_exception\u001b[39m=\u001b[39;49mlog_on_exception,\n\u001b[0;32m    530\u001b[0m     case_sensitive\u001b[39m=\u001b[39;49mcase_sensitive,\n\u001b[0;32m    531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    533\u001b[0m placeholders[query\u001b[39m.\u001b[39mquery_id_place_holder] \u001b[39m=\u001b[39m (\n\u001b[0;32m    534\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last \u001b[39melse\u001b[39;00m result\u001b[39m.\u001b[39mquery_id\n\u001b[0;32m    535\u001b[0m )\n\u001b[0;32m    536\u001b[0m result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor\u001b[39m.\u001b[39mdescription\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:101\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m     98\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:95\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     96\u001b[0m \u001b[39mexcept\u001b[39;00m ReauthenticationRequest \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m     98\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m     99\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:349\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[39m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_data_or_iter(\n\u001b[0;32m    350\u001b[0m         results_cursor\u001b[39m=\u001b[39;49mresults_cursor, to_pandas\u001b[39m=\u001b[39;49mto_pandas, to_iter\u001b[39m=\u001b[39;49mto_iter\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     \u001b[39mreturn\u001b[39;00m AsyncJob(\n\u001b[0;32m    354\u001b[0m         results_cursor[\u001b[39m\"\u001b[39m\u001b[39mqueryId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    355\u001b[0m         query,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    362\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\donsu\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:391\u001b[0m, in \u001b[0;36mServerConnection._to_data_or_iter\u001b[1;34m(self, results_cursor, to_pandas, to_iter)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_FAILED_FETCH_PANDAS(\n\u001b[0;32m    392\u001b[0m             \u001b[39mstr\u001b[39m(ex)\n\u001b[0;32m    393\u001b[0m         )\n\u001b[0;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     data_or_iter \u001b[39m=\u001b[39m (\n\u001b[0;32m    396\u001b[0m         \u001b[39miter\u001b[39m(results_cursor) \u001b[39mif\u001b[39;00m to_iter \u001b[39melse\u001b[39;00m results_cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[0;32m    397\u001b[0m     )\n",
      "\u001b[1;31mSnowparkFetchDataException\u001b[0m: (1406): Failed to fetch a Pandas Dataframe. The error is: Unable to allocate 6.19 MiB for an array with shape (2, 405760) and data type float64"
     ]
    }
   ],
   "source": [
    "final_df = final_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MENU_ITEM_ID</th>\n",
       "      <th>ORDER_ID</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>UNIT_PRICE</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>ORDER_TS</th>\n",
       "      <th>ORDER_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>436294482</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2020-04-24 15:22:16</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>436294482</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2020-04-24 15:22:16</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>436294482</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-04-24 15:22:16</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>436294482</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-04-24 15:22:16</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>436294483</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2020-04-24 15:22:33</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65483023</th>\n",
       "      <td>43</td>\n",
       "      <td>438422782</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2021-04-15 19:59:04</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65483024</th>\n",
       "      <td>44</td>\n",
       "      <td>438422782</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-04-15 19:59:04</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65483025</th>\n",
       "      <td>43</td>\n",
       "      <td>438422783</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2021-04-15 19:59:10</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65483026</th>\n",
       "      <td>42</td>\n",
       "      <td>438422784</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021-04-15 19:59:23</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65483027</th>\n",
       "      <td>46</td>\n",
       "      <td>438422784</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-04-15 19:59:23</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65483028 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MENU_ITEM_ID   ORDER_ID  QUANTITY  UNIT_PRICE  PRICE  \\\n",
       "0                   72  436294482         2         7.0   14.0   \n",
       "1                   73  436294482         1        12.0   12.0   \n",
       "2                   75  436294482         1         3.0    3.0   \n",
       "3                   71  436294482         1         9.0    9.0   \n",
       "4                   72  436294483         1         7.0    7.0   \n",
       "...                ...        ...       ...         ...    ...   \n",
       "65483023            43  438422782         1        15.0   15.0   \n",
       "65483024            44  438422782         1         2.0    2.0   \n",
       "65483025            43  438422783         1        15.0   15.0   \n",
       "65483026            42  438422784         1        10.0   10.0   \n",
       "65483027            46  438422784         1         3.0    3.0   \n",
       "\n",
       "                    ORDER_TS  ORDER_TOTAL  \n",
       "0        2020-04-24 15:22:16         38.0  \n",
       "1        2020-04-24 15:22:16         38.0  \n",
       "2        2020-04-24 15:22:16         38.0  \n",
       "3        2020-04-24 15:22:16         38.0  \n",
       "4        2020-04-24 15:22:33         49.0  \n",
       "...                      ...          ...  \n",
       "65483023 2021-04-15 19:59:04         71.0  \n",
       "65483024 2021-04-15 19:59:04         71.0  \n",
       "65483025 2021-04-15 19:59:10         15.0  \n",
       "65483026 2021-04-15 19:59:23         28.0  \n",
       "65483027 2021-04-15 19:59:23         28.0  \n",
       "\n",
       "[65483028 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('C:/Users/donsu/Downloads/product_team_orders_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values from the \"MENU_ITEM_ID\" column\n",
    "unique_menu_item_ids = final_df[\"MENU_ITEM_ID\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = menu_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert unique_menu_item_ids to a set for faster lookup\n",
    "unique_menu_item_ids_set = set(unique_menu_item_ids)\n",
    "\n",
    "# Filter the menu_df DataFrame\n",
    "menu_df = menu_df[menu_df[\"MENU_ITEM_ID\"].isin(unique_menu_item_ids_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string JSON data to a nested dictionary\n",
    "menu_df['MENU_ITEM_HEALTH_METRICS_OBJ'] = menu_df['MENU_ITEM_HEALTH_METRICS_OBJ'].apply(ast.literal_eval)\n",
    "\n",
    "# Use json_normalize to flatten the nested JSON data\n",
    "menu_item_metrics = pd.json_normalize(menu_df['MENU_ITEM_HEALTH_METRICS_OBJ'], record_path='menu_item_health_metrics')\n",
    "\n",
    "# Rename the columns\n",
    "menu_item_metrics = menu_item_metrics.rename(columns={\n",
    "    'is_dairy_free_flag': 'DAIRY_FREE',\n",
    "    'is_gluten_free_flag': 'GLUTEN_FREE',\n",
    "    'is_healthy_flag': 'HEALTHY',\n",
    "    'is_nut_free_flag': 'NUT_FREE'\n",
    "})\n",
    "\n",
    "# Replace 'Y' with 'Yes' and 'N' with 'No' in the DataFrame\n",
    "menu_item_metrics = menu_item_metrics.replace({'Y': 1, 'N': 0})\n",
    "\n",
    "# Concatenate the flattened DataFrame with the original DataFrame\n",
    "menu_df = pd.concat([menu_df, menu_item_metrics], axis=1)\n",
    "\n",
    "# Drop the original 'MENU_ITEM_HEALTH_METRICS_OBJ' and 'ingredients' column \n",
    "menu_df = menu_df.drop(columns=['MENU_ITEM_HEALTH_METRICS_OBJ', 'ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df.to_csv('C:/Users/donsu/Downloads/product_team_menu_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.95 GiB for an array with shape (4, 65483028) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Merge 'menu_filtered_df' and 'final_df' on the common column 'MENU_ITEM_ID'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_merged_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(menu_df, final_df, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMENU_ITEM_ID\u001b[39;49m\u001b[39m'\u001b[39;49m, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\merge.py:162\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m    148\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    149\u001b[0m         left,\n\u001b[0;32m    150\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         validate\u001b[39m=\u001b[39mvalidate,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result(copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\merge.py:811\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indicator_pre_merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright)\n\u001b[0;32m    809\u001b[0m join_index, left_indexer, right_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 811\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_and_concat(\n\u001b[0;32m    812\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[39m=\u001b[39;49mcopy\n\u001b[0;32m    813\u001b[0m )\n\u001b[0;32m    814\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_type)\n\u001b[0;32m    816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindicator:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\merge.py:802\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    800\u001b[0m left\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m llabels\n\u001b[0;32m    801\u001b[0m right\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m rlabels\n\u001b[1;32m--> 802\u001b[0m result \u001b[39m=\u001b[39m concat([left, right], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    803\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:204\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m concat_axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m _concat_managers_axis0(mgrs_indexers, axes, copy)\n\u001b[0;32m    206\u001b[0m mgrs_indexers \u001b[39m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers)\n\u001b[0;32m    208\u001b[0m concat_plans \u001b[39m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     _get_mgr_concatenation_plan(mgr, indexers) \u001b[39mfor\u001b[39;00m mgr, indexers \u001b[39min\u001b[39;00m mgrs_indexers\n\u001b[0;32m    210\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\concat.py:279\u001b[0m, in \u001b[0;36m_concat_managers_axis0\u001b[1;34m(mgrs_indexers, axes, copy)\u001b[0m\n\u001b[0;32m    277\u001b[0m     nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m \u001b[39melif\u001b[39;00m copy:\n\u001b[1;32m--> 279\u001b[0m     nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     \u001b[39m# by slicing instead of copy(deep=False), we get a new array\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[39m#  object, see test_concat_copy\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mgetitem_block(\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\blocks.py:540\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    538\u001b[0m refs: BlockValuesRefs \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m--> 540\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    541\u001b[0m     refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.95 GiB for an array with shape (4, 65483028) and data type int64"
     ]
    }
   ],
   "source": [
    "# # Merge 'menu_filtered_df' and 'final_df' on the common column 'MENU_ITEM_ID'\n",
    "# final_merged_df = pd.merge(menu_df, final_df, on='MENU_ITEM_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'ORDER_TS' column is in datetime format\n",
    "# final_merged_df['YEAR'] = final_merged_df['ORDER_TS'].dt.year\n",
    "# final_merged_df['MONTH'] = final_merged_df['ORDER_TS'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_merged_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
